{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock-price-prediction-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawar/tensorflow-notebooks/blob/master/stock_price_prediction_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EgaNRy_Rq0wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict stock prices  with Long short-term memory (LSTM)"
      ]
    },
    {
      "metadata": {
        "id": "S1WPMQ_LHNQU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This simple example will show you the "
      ]
    },
    {
      "metadata": {
        "id": "vurGbUjFwBFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install requirements\n",
        "We install Tensorflow 2.0 with GPU support first"
      ]
    },
    {
      "metadata": {
        "id": "QSG622MbwEOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "ac4d71bd-9927-40fe-a909-9d30f5d279ed"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fmR4lwm81Vbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "0fec97b4-9628-4764-dd7a-39c8843432be"
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas-datareader"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (2.18.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (0.22.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (1.10.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (1.22)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.19.2->pandas-datareader) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fcl6EKdTv51W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "Rui0kjzEvXTV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs are very powerful in sequence prediction problems. They can store past information."
      ]
    },
    {
      "metadata": {
        "id": "Ed7_c04-vz2f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "I use pandas-datareader to get the historical stock prices from Yahoo! finance  "
      ]
    },
    {
      "metadata": {
        "id": "Hh-pO_QXqysJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas_datareader import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D8-w0eEtwnaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tickers = 'AAPL'\n",
        "\n",
        "start_date = '1980-12-01'\n",
        "end_date = '2019-03-29'\n",
        "\n",
        "stock_data = data.get_data_yahoo(tickers, start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2HpcDRd38on",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "445e72d3-dae7-4815-bed5-6dcd0897fa10"
      },
      "cell_type": "code",
      "source": [
        "stock_data.head(10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1980-12-12</th>\n",
              "      <td>0.515625</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>117258400.0</td>\n",
              "      <td>0.023007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-15</th>\n",
              "      <td>0.488839</td>\n",
              "      <td>0.486607</td>\n",
              "      <td>0.488839</td>\n",
              "      <td>0.486607</td>\n",
              "      <td>43971200.0</td>\n",
              "      <td>0.021807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-16</th>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.450893</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.450893</td>\n",
              "      <td>26432000.0</td>\n",
              "      <td>0.020206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-17</th>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>21610400.0</td>\n",
              "      <td>0.020706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-18</th>\n",
              "      <td>0.477679</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>18362400.0</td>\n",
              "      <td>0.021307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-19</th>\n",
              "      <td>0.506696</td>\n",
              "      <td>0.504464</td>\n",
              "      <td>0.504464</td>\n",
              "      <td>0.504464</td>\n",
              "      <td>12157600.0</td>\n",
              "      <td>0.022607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-22</th>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.529018</td>\n",
              "      <td>0.529018</td>\n",
              "      <td>0.529018</td>\n",
              "      <td>9340800.0</td>\n",
              "      <td>0.023707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-23</th>\n",
              "      <td>0.553571</td>\n",
              "      <td>0.551339</td>\n",
              "      <td>0.551339</td>\n",
              "      <td>0.551339</td>\n",
              "      <td>11737600.0</td>\n",
              "      <td>0.024708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-24</th>\n",
              "      <td>0.582589</td>\n",
              "      <td>0.580357</td>\n",
              "      <td>0.580357</td>\n",
              "      <td>0.580357</td>\n",
              "      <td>12000800.0</td>\n",
              "      <td>0.026008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-26</th>\n",
              "      <td>0.636161</td>\n",
              "      <td>0.633929</td>\n",
              "      <td>0.633929</td>\n",
              "      <td>0.633929</td>\n",
              "      <td>13893600.0</td>\n",
              "      <td>0.028409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                High       Low      Open     Close       Volume  Adj Close\n",
              "Date                                                                      \n",
              "1980-12-12  0.515625  0.513393  0.513393  0.513393  117258400.0   0.023007\n",
              "1980-12-15  0.488839  0.486607  0.488839  0.486607   43971200.0   0.021807\n",
              "1980-12-16  0.453125  0.450893  0.453125  0.450893   26432000.0   0.020206\n",
              "1980-12-17  0.464286  0.462054  0.462054  0.462054   21610400.0   0.020706\n",
              "1980-12-18  0.477679  0.475446  0.475446  0.475446   18362400.0   0.021307\n",
              "1980-12-19  0.506696  0.504464  0.504464  0.504464   12157600.0   0.022607\n",
              "1980-12-22  0.531250  0.529018  0.529018  0.529018    9340800.0   0.023707\n",
              "1980-12-23  0.553571  0.551339  0.551339  0.551339   11737600.0   0.024708\n",
              "1980-12-24  0.582589  0.580357  0.580357  0.580357   12000800.0   0.026008\n",
              "1980-12-26  0.636161  0.633929  0.633929  0.633929   13893600.0   0.028409"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "6jzF16Qr4qOA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm only interested in *close* prices"
      ]
    },
    {
      "metadata": {
        "id": "Ns7Zcn2ZClZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "close_prices = stock_data.iloc[:, 1:2].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNaLuS1X5IVA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of course, some of the weekdays might be public holidays in which case no price will be available. For this reason, we will fill the missing prices with the latest available prices"
      ]
    },
    {
      "metadata": {
        "id": "P6GTsn7N5Nf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "#close_prices = close_prices.reindex(all_weekdays)\n",
        "#close_prices = close_prices.fillna(method='ffill')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTieT-gC5e-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(all_weekdays)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "06xfVbrD51Id",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset is now complete and free of missing values. Let's have a look to the data frame summary:"
      ]
    },
    {
      "metadata": {
        "id": "TR3_R4OF8pNo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature scaling"
      ]
    },
    {
      "metadata": {
        "id": "hFwPXHiU8rDv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_set = stock_data.loc[:, 'Close']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9TpiTPs9QtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(close_prices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmYp53pYHeQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs expect the data in a specific format, usually a 3D array (tensor). I start by creating data in 60 timesteps and converting it into an array using NumPy. Next, I convert the data into a 3D dimension array with X_train samples, 60 timestamps, and one feature at each step."
      ]
    },
    {
      "metadata": {
        "id": "HSZBYUcIDfWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 2035):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MQYJqsDGFYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the LSTM network\n",
        "Let's create a sequenced LSTM network with 50 units. Also the net includes some dropout layers with 0.2 which means that 20% of the layers will be dropped."
      ]
    },
    {
      "metadata": {
        "id": "057FLdkzDk1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tel113HiEcYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "dc5ab54b-a4f8-46ff-94e6-75f713a66973"
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(units = 50, return_sequences = True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(units = 50, return_sequences = True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(units = 50),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units = 1)\n",
        "])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0329 15:34:27.473214 140264946407296 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f91823cc438>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "W0329 15:34:27.498182 140264946407296 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f91ce822128>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "W0329 15:34:27.506912 140264946407296 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f91ce84bb00>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "W0329 15:34:27.511302 140264946407296 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f91802d1b38>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TVtDPeEZGmqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model will be compiled and optimize by the adam optimizer and set the loss function as mean_squarred_error"
      ]
    },
    {
      "metadata": {
        "id": "lj9k7R5UFrqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3575
        },
        "outputId": "5f9e0377-898a-4f92-c00c-dd097c4a012f"
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1975/1975 [==============================] - 5s 3ms/sample - loss: 1.5193e-05\n",
            "Epoch 2/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.8667e-06\n",
            "Epoch 3/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8390e-06\n",
            "Epoch 4/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 9.2604e-07\n",
            "Epoch 5/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 5.7586e-07\n",
            "Epoch 6/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 6.9929e-07\n",
            "Epoch 7/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.8375e-07\n",
            "Epoch 8/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.9812e-07\n",
            "Epoch 9/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 5.6590e-07\n",
            "Epoch 10/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 6.7315e-07\n",
            "Epoch 11/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.7882e-07\n",
            "Epoch 12/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.5509e-07\n",
            "Epoch 13/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.3987e-07\n",
            "Epoch 14/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.3916e-07\n",
            "Epoch 15/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 7.4475e-07\n",
            "Epoch 16/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.8332e-07\n",
            "Epoch 17/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.0749e-07\n",
            "Epoch 18/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.4366e-07\n",
            "Epoch 19/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.5559e-07\n",
            "Epoch 20/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.1883e-07\n",
            "Epoch 21/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.1844e-07\n",
            "Epoch 22/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.2457e-07\n",
            "Epoch 23/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.4503e-07\n",
            "Epoch 24/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.6516e-07\n",
            "Epoch 25/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9960e-07\n",
            "Epoch 26/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 5.2839e-07\n",
            "Epoch 27/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.5519e-07\n",
            "Epoch 28/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8017e-07\n",
            "Epoch 29/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.7858e-07\n",
            "Epoch 30/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.7449e-07\n",
            "Epoch 31/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.3426e-07\n",
            "Epoch 32/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9259e-07\n",
            "Epoch 33/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.0155e-07\n",
            "Epoch 34/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9893e-07\n",
            "Epoch 35/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.7578e-07\n",
            "Epoch 36/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.3323e-07\n",
            "Epoch 37/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.0047e-07\n",
            "Epoch 38/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.0056e-07\n",
            "Epoch 39/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.6854e-07\n",
            "Epoch 40/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.0016e-07\n",
            "Epoch 41/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.0857e-07\n",
            "Epoch 42/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.3314e-07\n",
            "Epoch 43/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.8516e-07\n",
            "Epoch 44/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9146e-07\n",
            "Epoch 45/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.4718e-07\n",
            "Epoch 46/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.0870e-07\n",
            "Epoch 47/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8123e-07\n",
            "Epoch 48/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8032e-07\n",
            "Epoch 49/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8809e-07\n",
            "Epoch 50/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 4.0363e-07\n",
            "Epoch 51/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.2806e-07\n",
            "Epoch 52/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.0884e-07\n",
            "Epoch 53/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2297e-07\n",
            "Epoch 54/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.7411e-07\n",
            "Epoch 55/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.6248e-07\n",
            "Epoch 56/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8646e-07\n",
            "Epoch 57/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.6404e-07\n",
            "Epoch 58/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9556e-07\n",
            "Epoch 59/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9833e-07\n",
            "Epoch 60/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.8183e-07\n",
            "Epoch 61/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.6268e-07\n",
            "Epoch 62/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.4561e-07\n",
            "Epoch 63/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.3695e-07\n",
            "Epoch 64/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 3.2914e-07\n",
            "Epoch 65/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.5828e-07\n",
            "Epoch 66/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.4850e-07\n",
            "Epoch 67/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2488e-07\n",
            "Epoch 68/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.1859e-07\n",
            "Epoch 69/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2383e-07\n",
            "Epoch 70/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2723e-07\n",
            "Epoch 71/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.4598e-07\n",
            "Epoch 72/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2984e-07\n",
            "Epoch 73/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2123e-07\n",
            "Epoch 74/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.3019e-07\n",
            "Epoch 75/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.0537e-07\n",
            "Epoch 76/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.1493e-07\n",
            "Epoch 77/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.4513e-07\n",
            "Epoch 78/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9985e-07\n",
            "Epoch 79/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.5414e-07\n",
            "Epoch 80/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.1298e-07\n",
            "Epoch 81/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.6494e-07\n",
            "Epoch 82/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.4956e-07\n",
            "Epoch 83/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.0408e-07\n",
            "Epoch 84/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.1311e-07\n",
            "Epoch 85/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.7872e-07\n",
            "Epoch 86/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.3220e-07\n",
            "Epoch 87/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.9255e-07\n",
            "Epoch 88/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9335e-07\n",
            "Epoch 89/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.0104e-07\n",
            "Epoch 90/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.5231e-07\n",
            "Epoch 91/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.7972e-07\n",
            "Epoch 92/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.0384e-07\n",
            "Epoch 93/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9510e-07\n",
            "Epoch 94/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.0983e-07\n",
            "Epoch 95/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.7797e-07\n",
            "Epoch 96/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9924e-07\n",
            "Epoch 97/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9663e-07\n",
            "Epoch 98/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9976e-07\n",
            "Epoch 99/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 2.2334e-07\n",
            "Epoch 100/100\n",
            "1975/1975 [==============================] - 4s 2ms/sample - loss: 1.9734e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f91547028d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}